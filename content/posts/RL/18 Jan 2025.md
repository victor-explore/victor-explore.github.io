---
title: "16 Jan 2025"
date:
draft: true
description:
tags: []
categories: []
author:
toc:
weight: 1
---
## Dynamic Programming Algorithm for Finite-Horizon MDPs

The Dynamic Programming (DP) algorithm is a powerful method used to find the optimal policy and the optimal cost-to-go function for a finite-horizon Markov Decision Process (MDP). It leverages the principle of optimality to break down a complex sequential decision-making problem into simpler, overlapping subproblems. For a finite-horizon MDP spanning $N$ stages, the DP algorithm computes the optimal decisions by working backward from the final stage to the initial stage.

For any initial state $x_0$ at stage 0, the optimal cost-to-go function for an $N$-stage problem, denoted as $J^*_0(x_0)$, is obtained by applying the following recursive algorithm:

1.  **Terminal Stage Cost (Base Case):**
    At the final stage $N$, no more decisions are made, and the process terminates. The cost at the terminal stage $N$ is simply given by the terminal cost function $g_N(x_N)$, which depends only on the state $x_N$ at stage $N$. Thus, the optimal cost-to-go at the final stage is initialized as:
    $$
    J_N(x_N) = g_N(x_N)
    $$
    This equation defines the base case for the backward recursion.

2.  **Recursive Cost-to-go Calculation (Backward Recursion):**
    For stages $k = N-1, N-2, \ldots, 0$, and for all possible states $x_k$ at stage $k$, the optimal cost-to-go function $J_k(x_k)$ is calculated recursively. This is done by considering all possible actions $a_k \in A(x_k)$ available in state $x_k$ at stage $k$. For each action $a_k$, we consider the immediate stage cost $g_k(x_k, a_k, x_{k+1})$ and the optimal cost-to-go from the next stage $J_{k+1}(x_{k+1})$, starting from the next state $x_{k+1}$. The next state $x_{k+1}$ is a random variable that depends on the current state $x_k$ and action $a_k$, following the state transition probability distribution $P(\cdot|x_k, a_k)$. We take the expectation over all possible next states $x_{k+1}$ and minimize this expected total cost over all possible actions $a_k \in A(x_k)$. Mathematically, this is expressed as:
    $$
    J_k(x_k) = \min_{a_k \in A(x_k)} \underbrace{E_{x_{k+1}} \left[ \underbrace{g_k(x_k,a_k,x_{k+1})}_{\substack{\text{Immediate stage cost} \\ \text{at stage } k}} + \underbrace{J_{k+1}(x_{k+1})}_{\substack{\text{Optimal cost-to-go} \\ \text{from stage } k+1}} \biggm\vert x_k, a_k \right]}_{\substack{\text{Expected total cost} \\ \text{from stage } k \text{ onwards}}}
    $$
    where $x_{k+1} \sim P(\cdot|x_k,a_k)$. This equation is the core of the dynamic programming algorithm, as it recursively defines the optimal cost-to-go function for each stage, working backward in time.

3.  **Optimal Policy Extraction:**
    Simultaneously with computing the optimal cost-to-go function $J_k(x_k)$, we also determine the optimal policy $\mu_k^*(x_k)$ at each stage $k$ and for each state $x_k$. The optimal policy $\mu_k^*(x_k)$ is the action $a_k$ that achieves the minimum expected total cost in the recursive equation from Step 2. In other words, for each state $x_k$ at stage $k$, the optimal action is the one that minimizes the sum of the immediate stage cost and the expected optimal cost-to-go from the next stage. Mathematically, the optimal policy at stage $k$ is given by:
    $$
    \mu_k^*(x_k) = \arg\min_{a_k \in A(x_k)} \underbrace{E_{x_{k+1}} \left[ \underbrace{g_k(x_k,a_k,x_{k+1})}_{\substack{\text{Immediate stage cost} \\ \text{at stage } k}} + \underbrace{J_{k+1}(x_{k+1})}_{\substack{\text{Optimal cost-to-go} \\ \text{from stage } k+1}} \biggm\vert x_k, a_k \right]}_{\substack{\text{Expected total cost} \\ \text{from stage } k \text{ onwards}}}
    $$
    This equation defines the optimal decision rule at each stage $k$ as a function of the current state $x_k$.

4.  **Optimal Policy Sequence:**
    By collecting the optimal decision rules $\mu_k^*(x_k)$ for all stages $k = 0, 1, \ldots, N-1$, we obtain the optimal policy for the entire finite-horizon MDP, denoted as $\pi^*$. The optimal policy $\pi^*$ is a sequence of functions, one for each stage, that specifies the optimal action to take in each state at each stage:
    $$
    \pi^* = \{\mu_0^*, \mu_1^*, \ldots, \mu_{N-1}^*\}
    $$

**Summary of the Dynamic Programming Algorithm:**

-   **Backward Recursion:** The algorithm operates by iterating backward in time, starting from the final stage $N$ and proceeding to the initial stage 0. This backward approach is crucial because the optimal decision at any stage depends on the optimal decisions at future stages.
-   **Optimal Cost-to-go Function:** It computes the optimal cost-to-go function $J_k(x_k)$ for each stage $k$ and state $x_k$. $J_k(x_k)$ represents the minimum expected cost that can be incurred from stage $k$ to the final stage $N$, starting from state $x_k$.
-   **Optimal Policy Determination:** For each stage $k$ and state $x_k$, the algorithm determines the optimal action $\mu_k^*(x_k)$ that minimizes the expected total cost from that stage onwards.
-   **Global Optimality:** Dynamic programming guarantees global optimality for finite-horizon MDPs under certain conditions (like optimal substructure and overlapping subproblems). By recursively optimizing at each stage, it ensures that the overall policy is optimal for the entire decision-making process.

