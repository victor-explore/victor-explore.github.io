---
title: "Stochastic Shortest Path Problems"
date: 2025-01-01
draft: true
description:
tags: []
categories: []
author:
toc:
weight: 1
---

## Stochastic Shortest Path Problems

Stochastic Shortest Path (SSP) problems are a class of Markov Decision Processes (MDPs) specifically designed to model scenarios where the objective is to reach a unique designated goal state, known as the terminal state, with minimum expected cost. Imagine navigating a maze with probabilistic transitions at each step; the SSP problem captures the essence of finding the most efficient path to the exit.

**Key Characteristics of SSP Problems:**

To define SSP problems with mathematical precision and intuitive clarity, we highlight the following key characteristics:

*   **Terminal State:** There exists a single terminal state, denoted as $x=0$. This state represents the desired goal or completion of the task.
    $$ x = 0 \quad \text{is the terminal state} $$

*   **Absorption at the Terminal State:** Once the system enters the terminal state ($x=0$), it remains there indefinitely, irrespective of the action chosen.  Mathematically, for any action $a \in \mathcal{A}(0)$ available in the terminal state, the transition probability to the terminal state is unity:
    $$ P(x'=0 \mid x=0, a) = 1, \quad \forall a \in \mathcal{A}(0) $$

*   **Zero Stage Cost at the Terminal State:**  No cost is incurred for any transition originating from or remaining in the terminal state. For any action $a \in \mathcal{A}(0)$, the stage cost is zero:
    $$ g(0, a, x') = 0, \quad \forall a \in \mathcal{A}(0), \forall x' \in \mathcal{X} $$

*   **Action Availability in Non-terminal States:** For all non-terminal states $x \neq 0$, a set of actions $\mathcal{A}(x)$ is available. These actions facilitate transitions to other states, potentially including the terminal state.
    $$ \mathcal{A}(x) \neq \emptyset, \quad \forall x \neq 0 $$

*   **No Discounting:** SSP problems consider the undiscounted cumulative cost. The discount factor $\gamma$ is set to 1, emphasizing the total cost incurred until the terminal state is reached without diminishing future costs.
    $$ \gamma = 1 $$

**Intuitive Examples Revisited:**

Let's revisit our examples to solidify these characteristics:

1.  **Robot Navigation:** A robot navigating to a charging station (terminal state $x=0$). Cost is energy or time.
2.  **Game Playing:** Reaching a winning state in chess (terminal state $x=0$). Cost is moves or resources.
3.  **Project Management:** Completing a project (terminal state $x=0$). Cost is time or resources.

**The Essence of "Shortest Path" in SSP:**

The term "shortest path" in SSP underscores the objective of finding a policy that minimizes the total *expected* cost accumulated *before* reaching the terminal state. With no discounting ($\gamma = 1$), we directly minimize the sum of stage costs along the path to the goal.

**Core Challenges in SSP Problems:**

Solving SSP problems effectively requires devising a policy that addresses the following core challenges:

*   **Guaranteeing Goal Achievement (Proper Policy):** The policy must ensure that the terminal state is reached with probability 1 from any starting non-terminal state. This is crucial for task completion. Such policies are termed "proper policies".
*   **Minimizing Expected Cost:** Among all proper policies, the objective is to find one that minimizes the expected cumulative cost to reach the terminal state. This is the "shortest path" aspect.
*   **Handling Stochasticity:** The policy must operate effectively under uncertainty in state transitions, accommodating the probabilistic outcomes of actions.

In summary, SSP problems offer a mathematically sound and intuitively appealing framework for modeling and solving sequential decision-making problems focused on reaching a goal state efficiently in uncertain environments.

## Mathematical Formulation of Stochastic Shortest Path Problems

In Stochastic Shortest Path (SSP) problems, our main objective is to find an optimal **policy** that dictates the best action to take in each state to reach the terminal state with the minimum expected total cost. To formalize this, we introduce the concept of the **optimal cost-to-go function**.

**Optimal Cost-to-Go Function: $J^*(x)$**

The optimal cost-to-go function, denoted as $J^*(x)$, represents the minimum expected cumulative cost to reach the terminal state starting from state $x$, assuming we consistently follow an optimal policy.  Mathematically, it is defined as:

$$
J^*(x) = \min_{\mu} J_\mu(x)
$$

where $J_\mu(x)$ is the cost-to-go function for a given policy $\mu$.  To understand $J^*(x)$ better, let's first define the optimal policy.

**Optimal Policy $\mu^*(x)$**

The optimal policy at state $x$, denoted as $\mu^*(x)$, is the action that minimizes the expected total cost from state $x$ onwards.  It is defined using the principle of optimality from dynamic programming.  For any non-terminal state $x \neq 0$, the optimal policy $\mu^*(x)$ is given by:

$$
\mu^*(x) = \arg\min_{a \in \mathcal{A}(x)} \underbrace{E_{x' \sim P(\cdot|x,a)} \left[ \underbrace{g(x,a,x')}_{\substack{\text{Immediate stage cost} \\ \text{for transition } (x,a,x')}} + \underbrace{J^*(x')}_{\substack{\text{Optimal cost-to-go} \\ \text{from next state } x'}} \biggm\vert x, a \right]}_{\substack{\text{Expected total cost} \\ \text{if action } a \text{ is taken in state } x}}
$$



Let's dissect this equation:

*   **$\mu^*(x)$**: Optimal action for state $x$ that minimizes expected total cost.

*   **$\arg\min_{a \in \mathcal{A}(x)}$**: Selects action $a$ from available actions $\mathcal{A}(x)$ in state $x$ that minimizes the following expression.

*   **$E_{x' \sim P(\cdot|x,a)} \left[ \cdot \biggm\vert x, a \right]$**: Expectation over next states $x'$ based on transition probability $P(x'|x,a)$ given current state $x$ and action $a$.

*   **$g(x,a,x')$**: Immediate stage cost for transition $(x,a,x')$.

*   **$J^*(x')$**: Optimal cost-to-go from next state $x'$, assuming optimal actions are taken subsequently.

**Intuitive Explanation of Optimal Policy Equation:**

The equation for $\mu^*(x)$ embodies the core idea of dynamic programming.  To find the best action in state $x$, we consider each possible action $a \in \mathcal{A}(x)$. For each action $a$, we evaluate the expected total cost, which consists of two parts:

1.  **Immediate Cost:** The cost $g(x,a,x')$ we incur right now for taking action $a$ and transitioning to the next state $x'$.
2.  **Future Optimal Cost:** The optimal cost-to-go $J^*(x')$ from the next state $x'$. This is the best possible cost we can expect to incur from state $x'$ onwards if we continue to act optimally.

By summing these two parts and taking the expectation over all possible next states $x'$, we get the total expected cost for choosing action $a$ in state $x$. The optimal policy $\mu^*(x)$ is then the action that minimizes this total expected cost.

**Optimal Cost-to-Go Function $J^*(x)$ in terms of Optimal Policy**

Once we have the optimal policy $\mu^*(x)$, we can express the optimal cost-to-go function $J^*(x)$ recursively as:

$$
J^*(x) = E_{x' \sim P(\cdot|x,\mu^*(x))} \left[ g(x,\mu^*(x),x') + J^*(x') \biggm\vert x, \mu^*(x) \right]
$$

For the terminal state $x=0$, the optimal cost-to-go is naturally zero, as we have already reached the goal:

$$
J^*(0) = 0
$$

This set of equations, for all states $x \in \mathcal{X}$, is known as **Bellman's Optimality Equation** for Stochastic Shortest Path problems. Solving this system of equations allows us to find both the optimal cost-to-go function $J^*(x)$ and the optimal policy $\mu^*(x)$.

**Cost-to-Go Function for a Given Stationary Policy: $J_\mu(x)$**

To evaluate the performance of a specific **stationary policy** $\mu$, which is a policy that chooses an action based only on the current state and not on the time step, we define the cost-to-go function for policy $\mu$, denoted as $J_\mu(x)$.  $J_\mu(x)$ represents the expected total cost when starting in state $x$ and consistently following policy $\mu$. It is mathematically defined as:

$$
J_\mu(x) = E\left[\sum_{k=0}^{\infty} g(x_k,\mu(x_k),x_{k+1}) \biggm\vert x_0 = x, \mu \right]
$$

This equation calculates the expected value of the sum of stage costs accumulated over an infinite number of steps, starting from state $x_0 = x$ and always applying the policy $\mu$.

**Explanation of $J_\mu(x)$ Equation:**

*   **$J_\mu(x)$**: Expected total cost for policy $\mu$ starting from state $x$.

*   **$E\left[\cdot \biggm\vert x_0 = x, \mu \right]$**: Expectation over state trajectories when starting at $x_0=x$ and following policy $\mu$.

*   **$\sum_{k=0}^{\infty} g(x_k,\mu(x_k),x_{k+1})$**: Sum of stage costs from time step $k=0$ until terminal state is reached.

**In essence, $J_\mu(x)$ answers the question:** "If we start at state $x$ and perpetually follow the policy $\mu$, what is the expected total cost we will incur until we reach the terminal state?".  By comparing $J_\mu(x)$ for different policies $\mu$, we can assess their effectiveness.  The optimal cost-to-go function $J^*(x)$ is then the minimum possible value of $J_\mu(x)$ across all possible policies $\mu$.

For a given stationary policy $\mu$, the cost-to-go function $J_\mu(x)$ can also be expressed recursively, similar to Bellman's equation, as:

$$
J_\mu(x) = E_{x' \sim P(\cdot|x,\mu(x))} \left[ g(x,\mu(x),x') + J_\mu(x') \biggm\vert x, \mu(x) \right]
$$

and for the terminal state:

$$
J_\mu(0) = 0
$$

This recursive form is useful for computational methods like policy evaluation, which is a step in algorithms like Policy Iteration and Value Iteration used to solve SSP problems.
## Proper and Improper Policies

In Stochastic Shortest Path (SSP) problems, policies are categorized as **proper** or **improper**. Proper policies are crucial because they guarantee reaching the terminal state, which is essential for task completion in SSP problems.

### Proper Policy

A stationary policy $\mu$ is **proper** if it ensures reaching the terminal state (state 0) with probability 1, regardless of the starting state.  This guarantees task completion.

Mathematically, a policy $\mu$ is proper if:
$$P(\text{Reach state } 0 \mid x_0 = i, \mu) = 1, \quad \forall i \in S$$
This means starting from any state $i$, following $\mu$ ensures reaching state 0 with certainty.

Alternatively, a policy $\mu$ is proper if there exists a finite time step $n \ge 1$ such that:
$$ \max_{i \in S} P(x_n \neq 0 \mid x_0 = i, \mu) < 1 $$
This condition implies that within $n$ steps, there's a guaranteed progress towards the terminal state from any starting state.

### Improper Policy

An **improper policy** fails to guarantee reaching the terminal state with probability 1.  This can lead to:

*   **Cycles:** The system may get trapped in loops, never reaching the terminal state.
*   **Non-Termination:** There's a non-zero probability of not reaching the terminal state from some starting states.
*   **Infinite Costs:**  Potentially infinite accumulated costs due to non-termination.

**Key Properties of Proper Policies:**

1.  **Recurrent Terminal State:**  A policy $\mu$ is proper if and only if the terminal state (state 0) is the only recurrent state in the induced Markov chain. All other states are transient.
2.  **Path to Terminal State:** From any non-terminal state, there must be a path to the terminal state with positive probability under a proper policy.

**Importance of Proper Policies:**

Proper policies are fundamental in SSP because:

*   **Task Completion:** They guarantee reaching the terminal state.
*   **Optimality:** Optimal policies in SSP problems must be proper. Improper policies are undesirable as they don't reliably solve the task.
*   **Finite Costs:** Proper policies ensure finite expected costs, which is essential for meaningful solutions. Improper policies can lead to infinite expected costs.

In essence, proper policies are essential for effectively solving SSP problems by ensuring task completion and enabling the search for optimal solutions with finite costs.

## Transition Probabilities Under Stationary Policies

Under a stationary policy $\mu$ in a Markov Decision Process (MDP) $\{X_k\}$, transitions between states are probabilistic. These probabilities, crucial for understanding system behavior, are defined based on the Markov property.

1.  **Markov Property:**
    The transition to state $j$ at time $k+1$ depends only on the current state $i$ and action $a$ at time $k$, not on prior history.
    $$P(X_{k+1}=j \mid X_k=i, Z_k=a, \dots) = P(X_{k+1}=j \mid X_k=i, Z_k=a)$$
    This simplifies MDP analysis by focusing solely on the present state and action for predicting the next state.

2.  **Transition Probabilities Under Policy $\mu$:**
    With a stationary policy $\mu$, action in state $i$ is always $\mu(i)$. The transition probability from state $i$ to $j$ under $\mu$, denoted as $P_\mu(i,j)$, is:
    $$P_\mu(i,j) \triangleq P(X_{k+1}=j \mid X_k=i, Z_k=\mu(i))$$
    $P_\mu(i,j)$ is the one-step transition probability from state $i$ to $j$ when following policy $\mu$.

These probabilities $P_\mu(i,j)$ must satisfy:

1.  **Non-negativity:**
    Probabilities are non-negative.
    $$P_\mu(i,j) \ge 0 \quad \forall i, j \in S$$

2.  **Normalization (for non-terminal states $i \in NT$):**
    From any non-terminal state, transitions to all possible next states sum to 1.
    $$\sum_{j \in S} P_\mu(i,j) = 1 \quad \forall i \in NT$$

3.  **Terminal State (State 0) Properties:**
    In Stochastic Shortest Path problems, state 0 is terminal.
    *   **Self-loop:**  Once in state 0, it remains there.
        $$P_\mu(0,0) = 1$$
    *   **No exit:** No transitions from state 0 to other states.
        $$P_\mu(0,j) = 0 \quad \forall j \neq 0$$

In summary, $P_\mu(i,j)$ define MDP dynamics under $\mu$. They are non-negative, normalized for non-terminal states, and ensure the terminal state is absorbing, which is key for Stochastic Shortest Path problem analysis.

## Probability of Not Reaching Terminal State

For a proper policy $\mu$, a crucial characteristic is that the probability of not reaching the terminal state diminishes over time. Let's analyze this probability after $n$ steps.

*   **Probability Definition**:
    Let $P(X_n \neq 0 \mid X_0 = i, \mu)$ denote the probability of not reaching the terminal state (state 0) after $n$ steps, given that we start in state $i$ and follow policy $\mu$.

*   **One-Step Probability of Not Reaching Terminal State**:
    Define $\rho_\mu$ as the maximum probability of transitioning from any non-terminal state to any other non-terminal state in a single step, under policy $\mu$. Mathematically,
    $$
    \rho_\mu = \max_{i \in NT} \sum_{j \in NT} P_\mu(i,j)
    $$
    where $NT$ is the set of non-terminal states, and $P_\mu(i,j)$ is the transition probability from state $i$ to state $j$ under policy $\mu$. For a proper policy, since there must be a path to the terminal state, the probability of staying within non-terminal states in one step must be less than 1, i.e., $\rho_\mu < 1$.

*   **Probability Bound after n Steps**:
    The probability of not reaching the terminal state after $n$ steps, starting from any initial non-terminal state $i$, is bounded by $\rho_\mu$ raised to the power of $n$:
    $$
    P(X_n \neq 0 \mid X_0 = i, \mu) \leq \rho_\mu^n
    $$

    *   **Intuitive Explanation**: In each step, from a non-terminal state, the probability of transitioning to another non-terminal state is at most $\rho_\mu$.  Therefore, after $n$ steps, the probability of remaining in non-terminal states for all $n$ steps is at most the product of these probabilities over each step, which is $\rho_\mu^n$.

*   **Geometric Decay**:
    Since $0 \leq \rho_\mu < 1$ for a proper policy, the term $\rho_\mu^n$ decreases geometrically as $n$ increases. This geometric decay signifies that as we take more steps, the probability of still being in a non-terminal state diminishes exponentially.

*   **Implication for Proper Policies**:
    This geometric decay is a fundamental characteristic of proper policies. It mathematically ensures that under a proper policy, the system is guaranteed to reach the terminal state with probability 1 as $n \rightarrow \infty$. This property is crucial for Stochastic Shortest Path problems because it ensures task completion and, as we will see next, finite expected costs.

In summary, the geometric decay of the probability of not reaching the terminal state is a hallmark of proper policies, guaranteeing eventual task completion and finite costs in Stochastic Shortest Path problems.

## Cost Function for Proper Policies

For a proper policy $\mu$, we aim to demonstrate that the expected total cost, starting from any state $i$, is finite. This is a crucial property for Stochastic Shortest Path (SSP) problems, ensuring that the problem is well-defined and solutions are meaningful.

1.  **Infinite Horizon Cost Function**:
    The infinite horizon cost function $J_\mu(i)$ for a policy $\mu$, starting from state $i$, is defined as the expected sum of stage costs incurred over an infinite number of steps:
    $$
    J_\mu(i) = E\left[\sum_{m=0}^{\infty} g(x_m,\mu(x_m),x_{m+1}) \mid x_0=i\right]
    $$
    where:
    - $x_0 = i$ is the initial state.
    - $x_{m+1}$ is the state at the next time step, transitioned from state $x_m$ by taking action $\mu(x_m)$ according to the transition probabilities $P(\cdot|x_m, \mu(x_m))$.
    - $g(x_m,\mu(x_m),x_{m+1})$ is the immediate cost incurred at stage $m$.

2.  **Proper Policies and Convergence to Terminal State**:
    For a proper policy $\mu$ in an SSP, we know that the probability of reaching the terminal state (state 0) approaches 1 as the number of steps increases. Mathematically, for any starting state $i$:
    $$
    \lim_{n\rightarrow\infty} P(x_n = 0 \mid x_0=i,\mu) = 1
    $$
    Equivalently, the probability of *not* reaching the terminal state after $n$ steps goes to 0:
    $$
    \lim_{n\rightarrow\infty} P(x_n \neq 0 \mid x_0=i,\mu) = 0
    $$
    Furthermore, we established that this probability decays geometrically:
    $$
    P(X_n \neq 0 \mid X_0 = i, \mu) \leq \rho_\mu^n
    $$
    where $\rho_\mu = \max_{i \in NT} \sum_{j \in NT} P_\mu(i,j) < 1$ for a proper policy.

3.  **Bounded Stage Costs**:
    We assume that the stage costs are bounded. That is, there exists a constant $C > 0$ such that for all states $i, j \in S$ and actions $a \in A(i)$:
    $$
    |g(i,a,j)| \leq C
    $$

4.  **Finiteness of Expected Total Cost**:
    To show that $J_\mu(i)$ is finite, we can bound its absolute value. Let $I_m$ be an indicator random variable such that $I_m = 1$ if $x_m \neq 0$ (the state at step $m$ is non-terminal) and $I_m = 0$ if $x_m = 0$ (the state at step $m$ is terminal). Since costs are only incurred in non-terminal states in typical SSP formulations (or cost from terminal state is zero), we can consider the cost at step $m$ to be effectively $g(x_m, \mu(x_m), x_{m+1}) \cdot I_m$.  Then, we can write:
    $$
    |J_\mu(i)| = \left| E\left[\sum_{m=0}^{\infty} g(x_m,\mu(x_m),x_{m+1}) \cdot I_m \mid x_0=i\right] \right|
    $$
    Using the triangle inequality for expectation and summation:
    $$
    |J_\mu(i)| \leq E\left[\sum_{m=0}^{\infty} |g(x_m,\mu(x_m),x_{m+1})| \cdot I_m \mid x_0=i\right] = \sum_{m=0}^{\infty} E\left[ |g(x_m,\mu(x_m),x_{m+1})| \cdot I_m \mid x_0=i\right]
    $$
    Since $|g(x_m,\mu(x_m),x_{m+1})| \leq C$, we have:
    $$
    |J_\mu(i)| \leq \sum_{m=0}^{\infty} E\left[ C \cdot I_m \mid x_0=i\right] = \sum_{m=0}^{\infty} C \cdot E\left[ I_m \mid x_0=i\right]
    $$
    The expectation of the indicator function $I_m$ is the probability that $x_m \neq 0$:
    $$
    E\left[ I_m \mid x_0=i\right] = P(x_m \neq 0 \mid x_0=i, \mu)
    $$
    Thus,
    $$
    |J_\mu(i)| \leq \sum_{m=0}^{\infty} C \cdot P(x_m \neq 0 \mid x_0=i, \mu)
    $$
    Using the geometric bound $P(X_m \neq 0 \mid X_0 = i, \mu) \leq \rho_\mu^m$:
    $$
    |J_\mu(i)| \leq \sum_{m=0}^{\infty} C \cdot \rho_\mu^m = C \sum_{m=0}^{\infty} \rho_\mu^m
    $$
    Since $0 \leq \rho_\mu < 1$ for a proper policy, the geometric series converges:
    $$
    \sum_{m=0}^{\infty} \rho_\mu^m = \frac{1}{1 - \rho_\mu}
    $$
    Therefore, the expected total cost is bounded by:
    $$
    |J_\mu(i)| \leq C \cdot \frac{1}{1 - \rho_\mu} < \infty
    $$
    This demonstrates that for any proper policy $\mu$ and any starting state $i$, the expected total cost $J_\mu(i)$ is finite, given bounded stage costs. This finiteness is a key characteristic of proper policies in Stochastic Shortest Path problems.


## Bellman Operators for Stochastic Shortest Path (SSP)

In Stochastic Shortest Path (SSP) problems, Bellman operators are essential tools rooted in dynamic programming. They provide a way to iteratively compute the optimal policy and evaluate the performance of a given policy by analyzing expected costs. Let's define the key components starting with the expected immediate cost.

### Expected Immediate Cost
First, consider an agent in a non-terminal state $i \in \{1, 2, ..., n\}$. When the agent chooses an action $u \in A(i)$, it immediately incurs a cost and transitions to a next state $j$. Since the transition to the next state is probabilistic, we define the expected immediate cost $\bar{g}(i,u)$ as the average cost incurred in state $i$ upon taking action $u$. This is calculated by summing over all possible next states $j$ (including the terminal state 0), weighted by their transition probabilities:

$$
\bar{g}(i,u) = \sum_{j=0}^n p_{ij}(u)g(i,u,j)
$$

where:
- $i \in \{1, 2, ..., n\}$ is the current non-terminal state.
- $u \in A(i)$ is the action chosen in state $i$.
- $j \in \{0, 1, ..., n\}$ is the next state, which can be any state including the terminal state 0.
- $p_{ij}(u)$ is the probability of transitioning from state $i$ to state $j$ when action $u$ is taken in state $i$.
- $g(i,u,j)$ is the cost incurred for transitioning from state $i$ to state $j$ when action $u$ is taken in state $i$.

### Cost-to-Go Function $J$

The cost-to-go function $J$ is a fundamental concept in Stochastic Shortest Path problems that quantifies the expected total cost to reach the terminal state from any starting state. For each non-terminal state $i$, it maps to a real number $J(i)$ representing this expected cost:

$$
J: NT \rightarrow \mathbb{R}, \quad NT = \{1, 2, ..., n\}
$$

where $NT$ is the set of non-terminal states. The function can be represented as a column vector:

$$
J = \begin{pmatrix} 
\underbrace{J(1)}_{\substack{\text{Expected cost} \\ \text{from state 1}}} \\ 
\underbrace{J(2)}_{\substack{\text{Expected cost} \\ \text{from state 2}}} \\ 
\vdots \\ 
\underbrace{J(n)}_{\substack{\text{Expected cost} \\ \text{from state n}}}
\end{pmatrix}
$$

This vector representation is particularly useful because:
- It enables efficient matrix operations in policy evaluation
- It makes it easy to track how costs change during value iteration
- It provides a compact way to represent the total expected costs for all states

### Optimal Bellman Operator ($T$): Discovering the Optimal Cost-to-Go Function

The optimal Bellman operator $T$ is a fundamental tool in Stochastic Shortest Path (SSP) problems that computes the optimal cost-to-go function $J^*$. It embodies Bellman's principle of optimality: regardless of the initial state and decision, the remaining decisions must form an optimal policy with respect to the resulting state.

For any cost-to-go function $J$, the operator $T$ produces a new function $TJ$ by evaluating, for each non-terminal state $i$, the best possible action considering both immediate and future costs. Mathematically:

$$
(TJ)(i) = \min_{u \in A(i)} \left\{ \bar{g}(i,u) + \sum_{j=1}^n p_{ij}(u)J(j) \right\} \quad \forall i \in \{1, 2, ..., n\}
$$

This equation can be decomposed as:

$$
\underbrace{(TJ)(i)}_{\substack{\text{Updated cost} \\ \text{estimate for state } i}} = \min_{u \in A(i)} \underbrace{\left\{ \underbrace{\bar{g}(i,u)}_{\substack{\text{Expected} \\ \text{immediate cost}}} + \underbrace{\sum_{j=1}^n p_{ij}(u)J(j)}_{\substack{\text{Expected future cost} \\ \text{weighted by transition} \\ \text{probabilities}}} \right\}}_{\substack{\text{Total expected cost} \\ \text{for action } u}}
$$

Key aspects of the optimal Bellman operator:

1. **State-Action Evaluation**: For each state $i$, it evaluates every possible action $u \in A(i)$ by:
   - Computing the expected immediate cost $\bar{g}(i,u)$
   - Calculating the expected future cost based on transition probabilities $p_{ij}(u)$ and current cost estimates $J(j)$
   - Summing these components to get the total expected cost

2. **Optimization**: It selects the action that minimizes the total expected cost, improving the cost estimate for state $i$

3. **Value Iteration**: The operator forms the core of the value iteration algorithm:
   - Start with an initial cost estimate $J_0$ (typically $J_0 = 0$)
   - Iteratively apply $T$: $J_{k+1} = TJ_k$
   - Converge to the optimal cost-to-go function: $\lim_{k \to \infty} J_k = J^*$

4. **Contraction Property**: Under proper conditions, $T$ is a contraction mapping, guaranteeing convergence to a unique fixed point $J^*$ that satisfies $TJ^* = J^*$

This operator provides a systematic way to discover both the optimal cost-to-go function and, implicitly, the optimal policy for SSP problems.
### Policy Evaluation Operator ($T_\mu$): Evaluating a Given Policy

The policy evaluation operator $T_\mu$ is a crucial tool for assessing the performance of a specific policy $\mu$ in Stochastic Shortest Path problems. It maps a cost-to-go function $J$ to a new cost-to-go function $T_\mu J$ by computing the expected total cost when following policy $\mu$ from each state.

For each non-terminal state $i$, the operator:
1. Takes the action $u = \mu(i)$ prescribed by policy $\mu$
2. Computes the expected immediate cost $\bar{g}_\mu(i) = \bar{g}(i,\mu(i))$
3. Adds the expected future cost based on transition probabilities

Mathematically, this is expressed as:

$$
(T_\mu J)(i) = \bar{g}_\mu(i) + \sum_{j=1}^n p_{ij}(\mu(i))J(j) \quad \forall i \in \{1, 2, ..., n\}
$$

This equation can be decomposed to understand its components:

$$
\underbrace{(T_\mu J)(i)}_{\substack{\text{Updated cost} \\ \text{estimate for state } i}} = \underbrace{\bar{g}_\mu(i)}_{\substack{\text{Expected} \\ \text{immediate cost}}} + \underbrace{\sum_{j=1}^n p_{ij}(\mu(i))J(j)}_{\substack{\text{Expected future cost} \\ \text{weighted by transition} \\ \text{probabilities}}}
$$

Key properties of the policy evaluation operator:

1. **Fixed Point**: The operator has a unique fixed point $J_\mu$ that satisfies $T_\mu J_\mu = J_\mu$. This fixed point represents the true cost-to-go function for policy $\mu$.

2. **Convergence**: Starting from any initial cost function $J_0$, repeated application of $T_\mu$ converges to $J_\mu$:
   $$
   \lim_{k \to \infty} T_\mu^k J_0 = J_\mu
   $$

3. **Contraction Mapping**: For proper policies, $T_\mu$ is a contraction mapping in the sup-norm, ensuring convergence regardless of the initial cost function.

4. **Linear Operation**: Unlike the optimal Bellman operator $T$, the policy evaluation operator $T_\mu$ is linear since the policy is fixed, making it computationally more tractable.

The policy evaluation operator forms the foundation of policy iteration algorithms, where it is used to accurately assess the performance of candidate policies during the improvement process.
### Transition Probability Matrix for Policy $\mu$ ($P_\mu$): Representing Policy Dynamics

The transition probability matrix $P_\mu$ provides a compact representation of how a system evolves under a fixed policy $\mu$ in a Stochastic Shortest Path (SSP) problem. For an SSP with $n$ non-terminal states, $P_\mu$ is an $n \times n$ matrix where each entry $[P_\mu]_{ij}$ represents the probability of transitioning from state $i$ to state $j$ when following policy $\mu$:

$$
[P_\mu]_{ij} = p_{ij}(\mu(i))
$$

The complete matrix structure is:

$$
P_\mu = \begin{pmatrix}
p_{11}(\mu(1)) & p_{12}(\mu(1)) & \cdots & p_{1n}(\mu(1)) \\
p_{21}(\mu(2)) & p_{22}(\mu(2)) & \cdots & p_{2n}(\mu(2)) \\
\vdots & \vdots & \ddots & \vdots \\
p_{n1}(\mu(n)) & p_{n2}(\mu(n)) & \cdots & p_{nn}(\mu(n))
\end{pmatrix}
$$

Key Properties and Characteristics:

1. **Sub-stochastic Nature**:
   - For each state $i$, the row sum satisfies:
     $$
     \sum_{j=1}^n [P_\mu]_{ij} = \sum_{j=1}^n p_{ij}(\mu(i)) \leq 1
     $$
   - The inequality is strict when there's a positive probability of reaching the terminal state
   - The "leakage probability" to the terminal state 0 is:
     $$
     p_{i0}(\mu(i)) = 1 - \sum_{j=1}^n p_{ij}(\mu(i))
     $$
   - Total probability conservation:
     $$
     \sum_{j=0}^n p_{ij}(\mu(i)) = 1
     $$

2. **Convergence Properties for Proper Policies**:
   - For a proper policy $\mu$, the matrix powers converge to zero:
     $$
     \lim_{k \to \infty} P_\mu^k = 0
     $$
   - This convergence is geometric, with rate determined by the policy's termination properties
   - Each power $k$ represents the probability distribution after $k$ steps
   - Convergence to zero reflects the guarantee of eventual termination

3. **Role in Policy Analysis**:
   - Enables efficient computation of expected costs through matrix operations
   - Facilitates analysis of policy performance and termination properties
   - Forms the basis for policy iteration and value iteration algorithms
   - Allows for direct comparison between different policies

The transition probability matrix $P_\mu$ serves as a fundamental tool in SSP analysis, providing both theoretical insights and practical computational advantages for policy evaluation and optimization.














